{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c61c2f7-2225-4da7-bd20-9d2ec6ad0553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using median trial length: 118\n",
      "Cascaded shape: (400, 2124)\n",
      "âœ… Separate PCA completed for Abhay and Arjun.\n",
      "Abhay: Eigenvectors (200, 2124), Eigenvalues 200\n",
      "Arjun: Eigenvectors (200, 2124), Eigenvalues 200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# === Column Definition ===\n",
    "expected_columns = [\n",
    "    'time', 'palm_position_x', 'palm_position_y', 'palm_position_z',\n",
    "    'palm_normal_x', 'palm_normal_y', 'palm_normal_z',\n",
    "    'palm_direction_x', 'palm_direction_y', 'palm_direction_z',\n",
    "    'hand_grab_angle', 'hand_grab_strength', 'hand_pinch_angle', 'hand_pinch_strength',\n",
    "    'thumb_extension', 'index_extension', 'middle_extension', 'ring_extension', 'pinky_extension'\n",
    "]\n",
    "\n",
    "# === Utilities ===\n",
    "def is_row_empty(row):\n",
    "    return all(pd.isna(cell) or (isinstance(cell, str) and cell.strip() == '') for cell in row)\n",
    "\n",
    "def trim_leading_empty_rows(df):\n",
    "    for i in range(len(df)):\n",
    "        if not is_row_empty(df.iloc[i]):\n",
    "            return df.iloc[i:].reset_index(drop=True)\n",
    "    return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "def trim_trailing_empty_rows(df):\n",
    "    for i in reversed(range(len(df))):\n",
    "        if not is_row_empty(df.iloc[i]):\n",
    "            return df.iloc[:i+1].reset_index(drop=True)\n",
    "    return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "def find_trial_split_index(df):\n",
    "    for i in range(len(df)):\n",
    "        if is_row_empty(df.iloc[i]):\n",
    "            if i+1 < len(df) and is_row_empty(df.iloc[i+1]):\n",
    "                return i\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def split_trials(df):\n",
    "    df = trim_leading_empty_rows(df)\n",
    "    split_idx = find_trial_split_index(df)\n",
    "    if split_idx is None:\n",
    "        return df.reset_index(drop=True), pd.DataFrame(columns=df.columns)\n",
    "    trial1 = df.iloc[:split_idx]\n",
    "    trial2 = df.iloc[split_idx+1:]\n",
    "    trial1 = trim_trailing_empty_rows(trial1)\n",
    "    trial2 = trim_leading_empty_rows(trial2)\n",
    "    trial2 = trim_trailing_empty_rows(trial2)\n",
    "    return trial1.reset_index(drop=True), trial2.reset_index(drop=True)\n",
    "\n",
    "def resample_by_time(df, time_col, target_rows):\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_numeric(df[time_col], errors='coerce')\n",
    "    df = df.dropna(subset=[time_col])\n",
    "    df = df.drop_duplicates(subset=time_col)\n",
    "    for col in df.columns:\n",
    "        if col != time_col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df = df.set_index(time_col)\n",
    "    new_time_index = np.linspace(df.index.min(), df.index.max(), target_rows)\n",
    "    df_resampled = df.reindex(new_time_index)\n",
    "    df_resampled = df_resampled.interpolate(method='linear', axis=0).reset_index()\n",
    "    df_resampled.rename(columns={'index': time_col}, inplace=True)\n",
    "    return df_resampled\n",
    "\n",
    "def safe_resample(df_list, time_col, target_rows):\n",
    "    return [\n",
    "        resample_by_time(df, time_col, target_rows)\n",
    "        for df in df_list\n",
    "        if time_col in df.columns and not df.empty\n",
    "    ]\n",
    "\n",
    "def cascade_single_trial(df):\n",
    "    df = df[[col for col in df.columns if not col.endswith(\"time\")]]\n",
    "    return pd.DataFrame([df.to_numpy().T.flatten()])\n",
    "\n",
    "# === MAIN PROCESSING ===\n",
    "input_folder = r\"C:\\Users\\Abhay\\Downloads\\ExportedSheets\"\n",
    "csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "all_abhay_trial1, all_abhay_trial2 = [], []\n",
    "all_arjun_trial1, all_arjun_trial2 = [], []\n",
    "\n",
    "for file in csv_files:\n",
    "    path = os.path.join(input_folder, file)\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        df = df.iloc[1:].reset_index(drop=True)\n",
    "        df = df.iloc[:, :39]  # 19 Abhay + 1 blank + 19 Arjun\n",
    "\n",
    "        abhay_df = df.iloc[:, :19].copy()\n",
    "        arjun_df = df.iloc[:, 20:].copy()\n",
    "\n",
    "        abhay_df.columns = [f\"Abhay_{col}\" for col in expected_columns]\n",
    "        arjun_df.columns = [f\"Arjun_{col}\" for col in expected_columns]\n",
    "\n",
    "        abhay_df.dropna(axis=1, how='all', inplace=True)\n",
    "        arjun_df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "        abhay_t1, abhay_t2 = split_trials(abhay_df)\n",
    "        arjun_t1, arjun_t2 = split_trials(arjun_df)\n",
    "\n",
    "        all_abhay_trial1.append(abhay_t1)\n",
    "        all_abhay_trial2.append(abhay_t2)\n",
    "        all_arjun_trial1.append(arjun_t1)\n",
    "        all_arjun_trial2.append(arjun_t2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# === Compute median trial length ===\n",
    "all_lengths = [len(df) for df in all_abhay_trial1 + all_abhay_trial2 + all_arjun_trial1 + all_arjun_trial2]\n",
    "median_len = int(np.median(all_lengths))\n",
    "print(f\"Using median trial length: {median_len}\")\n",
    "\n",
    "# === Resample all trials ===\n",
    "all_abhay_trial1 = safe_resample(all_abhay_trial1, 'Abhay_time', median_len)\n",
    "all_abhay_trial2 = safe_resample(all_abhay_trial2, 'Abhay_time', median_len)\n",
    "all_arjun_trial1 = safe_resample(all_arjun_trial1, 'Arjun_time', median_len)\n",
    "all_arjun_trial2 = safe_resample(all_arjun_trial2, 'Arjun_time', median_len)\n",
    "\n",
    "# === Cascade each trial as its own row ===\n",
    "cascaded_trials = []\n",
    "for trial in all_abhay_trial1 + all_abhay_trial2 + all_arjun_trial1 + all_arjun_trial2:\n",
    "    cascaded_trials.append(cascade_single_trial(trial))\n",
    "\n",
    "full_df = pd.concat(cascaded_trials, ignore_index=True)\n",
    "print(f\"Cascaded shape: {full_df.shape}\")\n",
    "\n",
    "# === Split by person\n",
    "num_trials = len(full_df)\n",
    "half = num_trials // 2\n",
    "df_abhay = full_df.iloc[:half].reset_index(drop=True)\n",
    "df_arjun = full_df.iloc[half:].reset_index(drop=True)\n",
    "\n",
    "# === Normalize and apply PCA (Abhay) ===\n",
    "scaler_abhay = MinMaxScaler()\n",
    "normalized_abhay = pd.DataFrame(scaler_abhay.fit_transform(df_abhay))\n",
    "pca_abhay = PCA()\n",
    "pca_abhay.fit(normalized_abhay)\n",
    "eigenvectors_abhay = pd.DataFrame(pca_abhay.components_)\n",
    "eigenvalues_abhay = pd.Series(pca_abhay.explained_variance_)\n",
    "\n",
    "# === Normalize and apply PCA (Arjun) ===\n",
    "scaler_arjun = MinMaxScaler()\n",
    "normalized_arjun = pd.DataFrame(scaler_arjun.fit_transform(df_arjun))\n",
    "pca_arjun = PCA()\n",
    "pca_arjun.fit(normalized_arjun)\n",
    "eigenvectors_arjun = pd.DataFrame(pca_arjun.components_)\n",
    "eigenvalues_arjun = pd.Series(pca_arjun.explained_variance_)\n",
    "\n",
    "# === Display output ===\n",
    "print(\"âœ… Separate PCA completed for Abhay and Arjun.\")\n",
    "print(f\"Abhay: Eigenvectors {eigenvectors_abhay.shape}, Eigenvalues {len(eigenvalues_abhay)}\")\n",
    "print(f\"Arjun: Eigenvectors {eigenvectors_arjun.shape}, Eigenvalues {len(eigenvalues_arjun)}\")\n",
    "\n",
    "# === Optional: view in notebook ===\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49e38d7b-4c9d-4ae2-8030-2502dd706c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using median trial length: 118\n",
      "Cascaded Abhay: (200, 2124), Arjun: (200, 2124)\n",
      "Abhay: Components to reach 90% variance: 12\n",
      "Arjun: Components to reach 90% variance: 12\n",
      "âœ… PCA complete.\n",
      "Abhay: Train (140, 2124), Eigenvectors (140, 2124), Eigenvalues 140\n",
      "Arjun: Train (140, 2124), Eigenvectors (140, 2124), Eigenvalues 140\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# === Config ===\n",
    "expected_columns = [\n",
    "    'time', 'palm_position_x', 'palm_position_y', 'palm_position_z',\n",
    "    'palm_normal_x', 'palm_normal_y', 'palm_normal_z',\n",
    "    'palm_direction_x', 'palm_direction_y', 'palm_direction_z',\n",
    "    'hand_grab_angle', 'hand_grab_strength', 'hand_pinch_angle', 'hand_pinch_strength',\n",
    "    'thumb_extension', 'index_extension', 'middle_extension', 'ring_extension', 'pinky_extension'\n",
    "]\n",
    "input_folder = r\"C:\\Users\\Abhay\\Downloads\\ExportedSheets\"\n",
    "\n",
    "# === Utility Functions ===\n",
    "def is_row_empty(row):\n",
    "    return all(pd.isna(cell) or (isinstance(cell, str) and cell.strip() == '') for cell in row)\n",
    "\n",
    "def trim_leading_empty_rows(df):\n",
    "    for i in range(len(df)):\n",
    "        if not is_row_empty(df.iloc[i]):\n",
    "            return df.iloc[i:].reset_index(drop=True)\n",
    "    return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "def trim_trailing_empty_rows(df):\n",
    "    for i in reversed(range(len(df))):\n",
    "        if not is_row_empty(df.iloc[i]):\n",
    "            return df.iloc[:i+1].reset_index(drop=True)\n",
    "    return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "def find_trial_split_index(df):\n",
    "    for i in range(len(df)):\n",
    "        if is_row_empty(df.iloc[i]):\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def split_trials(df):\n",
    "    df = trim_leading_empty_rows(df)\n",
    "    split_idx = find_trial_split_index(df)\n",
    "    if split_idx is None:\n",
    "        return df.reset_index(drop=True), pd.DataFrame(columns=df.columns)\n",
    "    trial1 = df.iloc[:split_idx]\n",
    "    trial2 = df.iloc[split_idx+1:]\n",
    "    trial1 = trim_trailing_empty_rows(trial1)\n",
    "    trial2 = trim_leading_empty_rows(trial2)\n",
    "    trial2 = trim_trailing_empty_rows(trial2)\n",
    "    return trial1.reset_index(drop=True), trial2.reset_index(drop=True)\n",
    "\n",
    "def resample_by_time(df, time_col, target_rows):\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_numeric(df[time_col], errors='coerce')\n",
    "    df = df.dropna(subset=[time_col])\n",
    "    df = df.drop_duplicates(subset=time_col)\n",
    "    for col in df.columns:\n",
    "        if col != time_col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df = df.set_index(time_col)\n",
    "    new_time_index = np.linspace(df.index.min(), df.index.max(), target_rows)\n",
    "    df_resampled = df.reindex(new_time_index)\n",
    "    df_resampled = df_resampled.interpolate(method='linear', axis=0).reset_index()\n",
    "    df_resampled.rename(columns={'index': time_col}, inplace=True)\n",
    "    return df_resampled\n",
    "\n",
    "def safe_resample(df_list, time_col, target_rows):\n",
    "    return [\n",
    "        resample_by_time(df, time_col, target_rows)\n",
    "        for df in df_list\n",
    "        if time_col in df.columns and not df.empty\n",
    "    ]\n",
    "\n",
    "def cascade_single_trial(df):\n",
    "    df = df[[col for col in df.columns if not col.endswith(\"time\")]]\n",
    "    return pd.DataFrame([df.to_numpy().T.flatten()])\n",
    "\n",
    "# === Load and Process All Files ===\n",
    "csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "abhay_trials, arjun_trials = [], []\n",
    "\n",
    "for file in csv_files:\n",
    "    path = os.path.join(input_folder, file)\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        df = df.iloc[1:].reset_index(drop=True)\n",
    "        df = df.iloc[:, :39]\n",
    "\n",
    "        abhay_df = df.iloc[:, :19].copy()\n",
    "        arjun_df = df.iloc[:, 20:].copy()\n",
    "\n",
    "        abhay_df.columns = [f\"Abhay_{col}\" for col in expected_columns]\n",
    "        arjun_df.columns = [f\"Arjun_{col}\" for col in expected_columns]\n",
    "\n",
    "        abhay_df.dropna(axis=1, how='all', inplace=True)\n",
    "        arjun_df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "        abhay_t1, abhay_t2 = split_trials(abhay_df)\n",
    "        arjun_t1, arjun_t2 = split_trials(arjun_df)\n",
    "\n",
    "        abhay_trials.extend([abhay_t1, abhay_t2])\n",
    "        arjun_trials.extend([arjun_t1, arjun_t2])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# === Resample all trials ===\n",
    "all_lengths = [len(df) for df in abhay_trials + arjun_trials]\n",
    "median_len = int(np.median(all_lengths))\n",
    "print(f\"Using median trial length: {median_len}\")\n",
    "\n",
    "abhay_trials = safe_resample(abhay_trials, 'Abhay_time', median_len)\n",
    "arjun_trials = safe_resample(arjun_trials, 'Arjun_time', median_len)\n",
    "\n",
    "# === Cascade Each Trial Individually ===\n",
    "cascaded_abhay = pd.concat([cascade_single_trial(trial) for trial in abhay_trials], ignore_index=True)\n",
    "cascaded_arjun = pd.concat([cascade_single_trial(trial) for trial in arjun_trials], ignore_index=True)\n",
    "print(f\"Cascaded Abhay: {cascaded_abhay.shape}, Arjun: {cascaded_arjun.shape}\")\n",
    "\n",
    "# === Normalize ===\n",
    "scaler_abhay = MinMaxScaler()\n",
    "normalized_abhay = pd.DataFrame(scaler_abhay.fit_transform(cascaded_abhay))\n",
    "\n",
    "scaler_arjun = MinMaxScaler()\n",
    "normalized_arjun = pd.DataFrame(scaler_arjun.fit_transform(cascaded_arjun))\n",
    "\n",
    "# === Train/Test Split (70/30) ===\n",
    "train_abhay = normalized_abhay.iloc[:140].reset_index(drop=True)\n",
    "test_abhay = normalized_abhay.iloc[140:].reset_index(drop=True)\n",
    "\n",
    "train_arjun = normalized_arjun.iloc[:140].reset_index(drop=True)\n",
    "test_arjun = normalized_arjun.iloc[140:].reset_index(drop=True)\n",
    "\n",
    "# === PCA (only on training sets) ===\n",
    "pca_abhay = PCA()\n",
    "pca_abhay.fit(train_abhay)\n",
    "eigenvectors_abhay = pd.DataFrame(pca_abhay.components_)\n",
    "eigenvalues_abhay = pd.Series(pca_abhay.explained_variance_)\n",
    "# Explained variance ratio (as %)\n",
    "explained_var_ratio = pca_abhay.explained_variance_ratio_\n",
    "\n",
    "# Cumulative variance\n",
    "cumulative_variance = np.cumsum(explained_var_ratio)\n",
    "\n",
    "# Find number of components needed for ~90% variance\n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "\n",
    "print(f\"Abhay: Components to reach 90% variance: {n_components_90}\")\n",
    "\n",
    "\n",
    "pca_arjun = PCA()\n",
    "pca_arjun.fit(train_arjun)\n",
    "eigenvectors_arjun = pd.DataFrame(pca_arjun.components_)\n",
    "eigenvalues_arjun = pd.Series(pca_arjun.explained_variance_)\n",
    "# Explained variance ratio (as %)\n",
    "explained_var_ratio_2 = pca_arjun.explained_variance_ratio_\n",
    "\n",
    "# Cumulative variance\n",
    "cumulative_variance_2 = np.cumsum(explained_var_ratio_2)\n",
    "\n",
    "# Find number of components needed for ~90% variance\n",
    "n_components_90_2 = np.argmax(cumulative_variance_2 >= 0.90) + 1\n",
    "\n",
    "print(f\"Arjun: Components to reach 90% variance: {n_components_90_2}\")\n",
    "\n",
    "# === Output ===\n",
    "print(\"âœ… PCA complete.\")\n",
    "print(f\"Abhay: Train {train_abhay.shape}, Eigenvectors {eigenvectors_abhay.shape}, Eigenvalues {len(eigenvalues_abhay)}\")\n",
    "print(f\"Arjun: Train {train_arjun.shape}, Eigenvectors {eigenvectors_arjun.shape}, Eigenvalues {len(eigenvalues_arjun)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d8a4cd9-d0f8-475e-9834-11a9805d28d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using median trial length: 118\n",
      "Cascaded Abhay: (200, 2124), Arjun: (200, 2124)\n",
      "Abhay PCA components (90% var): 12\n",
      "Arjun PCA components (90% var): 12\n",
      "\n",
      "âœ… Reconstruction Complete\n",
      "Abhay: Test (60, 2124) -> Reconstructed (60, 2124)\n",
      "Arjun: Test (60, 2124) -> Reconstructed (60, 2124)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# === Config ===\n",
    "expected_columns = [\n",
    "    'time', 'palm_position_x', 'palm_position_y', 'palm_position_z',\n",
    "    'palm_normal_x', 'palm_normal_y', 'palm_normal_z',\n",
    "    'palm_direction_x', 'palm_direction_y', 'palm_direction_z',\n",
    "    'hand_grab_angle', 'hand_grab_strength', 'hand_pinch_angle', 'hand_pinch_strength',\n",
    "    'thumb_extension', 'index_extension', 'middle_extension', 'ring_extension', 'pinky_extension'\n",
    "]\n",
    "input_folder = r\"C:\\Users\\Abhay\\Downloads\\ExportedSheets\"\n",
    "\n",
    "# === Utility Functions ===\n",
    "def is_row_empty(row):\n",
    "    return all(pd.isna(cell) or (isinstance(cell, str) and cell.strip() == '') for cell in row)\n",
    "\n",
    "def trim_leading_empty_rows(df):\n",
    "    for i in range(len(df)):\n",
    "        if not is_row_empty(df.iloc[i]):\n",
    "            return df.iloc[i:].reset_index(drop=True)\n",
    "    return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "def trim_trailing_empty_rows(df):\n",
    "    for i in reversed(range(len(df))):\n",
    "        if not is_row_empty(df.iloc[i]):\n",
    "            return df.iloc[:i+1].reset_index(drop=True)\n",
    "    return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "def find_trial_split_index(df):\n",
    "    for i in range(len(df)):\n",
    "        if is_row_empty(df.iloc[i]):\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def split_trials(df):\n",
    "    df = trim_leading_empty_rows(df)\n",
    "    split_idx = find_trial_split_index(df)\n",
    "    if split_idx is None:\n",
    "        return df.reset_index(drop=True), pd.DataFrame(columns=df.columns)\n",
    "    trial1 = df.iloc[:split_idx]\n",
    "    trial2 = df.iloc[split_idx+1:]\n",
    "    trial1 = trim_trailing_empty_rows(trial1)\n",
    "    trial2 = trim_leading_empty_rows(trial2)\n",
    "    trial2 = trim_trailing_empty_rows(trial2)\n",
    "    return trial1.reset_index(drop=True), trial2.reset_index(drop=True)\n",
    "\n",
    "def resample_by_time(df, time_col, target_rows):\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_numeric(df[time_col], errors='coerce')\n",
    "    df = df.dropna(subset=[time_col])\n",
    "    df = df.drop_duplicates(subset=time_col)\n",
    "    for col in df.columns:\n",
    "        if col != time_col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df = df.set_index(time_col)\n",
    "    new_time_index = np.linspace(df.index.min(), df.index.max(), target_rows)\n",
    "    df_resampled = df.reindex(new_time_index)\n",
    "    df_resampled = df_resampled.interpolate(method='linear', axis=0).reset_index()\n",
    "    df_resampled.rename(columns={'index': time_col}, inplace=True)\n",
    "    return df_resampled\n",
    "\n",
    "def safe_resample(df_list, time_col, target_rows):\n",
    "    return [\n",
    "        resample_by_time(df, time_col, target_rows)\n",
    "        for df in df_list\n",
    "        if time_col in df.columns and not df.empty\n",
    "    ]\n",
    "\n",
    "def cascade_single_trial(df):\n",
    "    df = df[[col for col in df.columns if not col.endswith(\"time\")]]\n",
    "    return pd.DataFrame([df.to_numpy().T.flatten()])\n",
    "\n",
    "def get_k_components(pca, threshold=0.90):\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    k = np.argmax(cumulative_variance >= threshold) + 1\n",
    "    return k\n",
    "\n",
    "def reconstruct_from_pca(test_df, pca, k_components):\n",
    "    eigenvectors_k = pca.components_[:k_components, :]\n",
    "    projected = np.dot(test_df, eigenvectors_k.T)\n",
    "    reconstructed = np.dot(projected, eigenvectors_k)\n",
    "    return pd.DataFrame(reconstructed)\n",
    "\n",
    "# === Load and Process All Files ===\n",
    "csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "abhay_trials, arjun_trials = [], []\n",
    "\n",
    "for file in csv_files:\n",
    "    path = os.path.join(input_folder, file)\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        df = df.iloc[1:].reset_index(drop=True)\n",
    "        df = df.iloc[:, :39]\n",
    "\n",
    "        abhay_df = df.iloc[:, :19].copy()\n",
    "        arjun_df = df.iloc[:, 20:].copy()\n",
    "\n",
    "        abhay_df.columns = [f\"Abhay_{col}\" for col in expected_columns]\n",
    "        arjun_df.columns = [f\"Arjun_{col}\" for col in expected_columns]\n",
    "\n",
    "        abhay_df.dropna(axis=1, how='all', inplace=True)\n",
    "        arjun_df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "        abhay_t1, abhay_t2 = split_trials(abhay_df)\n",
    "        arjun_t1, arjun_t2 = split_trials(arjun_df)\n",
    "\n",
    "        abhay_trials.extend([abhay_t1, abhay_t2])\n",
    "        arjun_trials.extend([arjun_t1, arjun_t2])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# === Resample all trials ===\n",
    "all_lengths = [len(df) for df in abhay_trials + arjun_trials]\n",
    "median_len = int(np.median(all_lengths))\n",
    "print(f\"Using median trial length: {median_len}\")\n",
    "\n",
    "abhay_trials = safe_resample(abhay_trials, 'Abhay_time', median_len)\n",
    "arjun_trials = safe_resample(arjun_trials, 'Arjun_time', median_len)\n",
    "\n",
    "# === Cascade Each Trial Individually ===\n",
    "cascaded_abhay = pd.concat([cascade_single_trial(trial) for trial in abhay_trials], ignore_index=True)\n",
    "cascaded_arjun = pd.concat([cascade_single_trial(trial) for trial in arjun_trials], ignore_index=True)\n",
    "print(f\"Cascaded Abhay: {cascaded_abhay.shape}, Arjun: {cascaded_arjun.shape}\")\n",
    "\n",
    "# === Normalize ===\n",
    "scaler_abhay = MinMaxScaler()\n",
    "normalized_abhay = pd.DataFrame(scaler_abhay.fit_transform(cascaded_abhay))\n",
    "\n",
    "scaler_arjun = MinMaxScaler()\n",
    "normalized_arjun = pd.DataFrame(scaler_arjun.fit_transform(cascaded_arjun))\n",
    "\n",
    "# === Train/Test Split (70/30 per person) ===\n",
    "train_abhay = normalized_abhay.iloc[:140].reset_index(drop=True)\n",
    "test_abhay = normalized_abhay.iloc[140:].reset_index(drop=True)\n",
    "\n",
    "train_arjun = normalized_arjun.iloc[:140].reset_index(drop=True)\n",
    "test_arjun = normalized_arjun.iloc[140:].reset_index(drop=True)\n",
    "\n",
    "# === PCA Training ===\n",
    "pca_abhay = PCA().fit(train_abhay)\n",
    "pca_arjun = PCA().fit(train_arjun)\n",
    "\n",
    "# === Determine top-k for 90% variance ===\n",
    "k_abhay = get_k_components(pca_abhay, threshold=0.90)\n",
    "k_arjun = get_k_components(pca_arjun, threshold=0.90)\n",
    "print(f\"Abhay PCA components (90% var): {k_abhay}\")\n",
    "print(f\"Arjun PCA components (90% var): {k_arjun}\")\n",
    "\n",
    "# === Reconstruct test data ===\n",
    "reconstructed_abhay = reconstruct_from_pca(test_abhay, pca_abhay, k_abhay)\n",
    "reconstructed_arjun = reconstruct_from_pca(test_arjun, pca_arjun, k_arjun)\n",
    "\n",
    "# === Final Output ===\n",
    "print(\"\\nâœ… Reconstruction Complete\")\n",
    "print(f\"Abhay: Test {test_abhay.shape} -> Reconstructed {reconstructed_abhay.shape}\")\n",
    "print(f\"Arjun: Test {test_arjun.shape} -> Reconstructed {reconstructed_arjun.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd252e9d-cd6a-46ce-853b-dba8be7c239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using median trial length: 118\n",
      "Cascaded Abhay: (200, 2124), Arjun: (200, 2124)\n",
      "Abhay PCA components (90% var): 12\n",
      "Arjun PCA components (90% var): 12\n",
      "\n",
      "âœ… PCA & Reconstruction Complete\n",
      "Abhay: Test (60, 2124) -> Reconstructed (60, 2124)\n",
      "Arjun: Test (60, 2124) -> Reconstructed (60, 2124)\n",
      "\n",
      "ðŸ“Š Abhay SSD: 16847.2693, MSE: 0.132198\n",
      "ðŸ“Š Arjun SSD: 16287.7298, MSE: 0.127807\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# === Config ===\n",
    "expected_columns = [\n",
    "    'time', 'palm_position_x', 'palm_position_y', 'palm_position_z',\n",
    "    'palm_normal_x', 'palm_normal_y', 'palm_normal_z',\n",
    "    'palm_direction_x', 'palm_direction_y', 'palm_direction_z',\n",
    "    'hand_grab_angle', 'hand_grab_strength', 'hand_pinch_angle', 'hand_pinch_strength',\n",
    "    'thumb_extension', 'index_extension', 'middle_extension', 'ring_extension', 'pinky_extension'\n",
    "]\n",
    "input_folder = r\"C:\\Users\\Abhay\\Downloads\\ExportedSheets\"\n",
    "\n",
    "# === Utility Functions ===\n",
    "def is_row_empty(row):\n",
    "    return all(pd.isna(cell) or (isinstance(cell, str) and cell.strip() == '') for cell in row)\n",
    "\n",
    "def trim_leading_empty_rows(df):\n",
    "    for i in range(len(df)):\n",
    "        if not is_row_empty(df.iloc[i]):\n",
    "            return df.iloc[i:].reset_index(drop=True)\n",
    "    return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "def trim_trailing_empty_rows(df):\n",
    "    for i in reversed(range(len(df))):\n",
    "        if not is_row_empty(df.iloc[i]):\n",
    "            return df.iloc[:i+1].reset_index(drop=True)\n",
    "    return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "def find_trial_split_index(df):\n",
    "    for i in range(len(df)):\n",
    "        if is_row_empty(df.iloc[i]):\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def split_trials(df):\n",
    "    df = trim_leading_empty_rows(df)\n",
    "    split_idx = find_trial_split_index(df)\n",
    "    if split_idx is None:\n",
    "        return df.reset_index(drop=True), pd.DataFrame(columns=df.columns)\n",
    "    trial1 = df.iloc[:split_idx]\n",
    "    trial2 = df.iloc[split_idx+1:]\n",
    "    trial1 = trim_trailing_empty_rows(trial1)\n",
    "    trial2 = trim_leading_empty_rows(trial2)\n",
    "    trial2 = trim_trailing_empty_rows(trial2)\n",
    "    return trial1.reset_index(drop=True), trial2.reset_index(drop=True)\n",
    "\n",
    "def resample_by_time(df, time_col, target_rows):\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_numeric(df[time_col], errors='coerce')\n",
    "    df = df.dropna(subset=[time_col])\n",
    "    df = df.drop_duplicates(subset=time_col)\n",
    "    for col in df.columns:\n",
    "        if col != time_col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df = df.set_index(time_col)\n",
    "    new_time_index = np.linspace(df.index.min(), df.index.max(), target_rows)\n",
    "    df_resampled = df.reindex(new_time_index)\n",
    "    df_resampled = df_resampled.interpolate(method='linear', axis=0).reset_index()\n",
    "    df_resampled.rename(columns={'index': time_col}, inplace=True)\n",
    "    return df_resampled\n",
    "\n",
    "def safe_resample(df_list, time_col, target_rows):\n",
    "    return [\n",
    "        resample_by_time(df, time_col, target_rows)\n",
    "        for df in df_list\n",
    "        if time_col in df.columns and not df.empty\n",
    "    ]\n",
    "\n",
    "def cascade_single_trial(df):\n",
    "    df = df[[col for col in df.columns if not col.endswith(\"time\")]]\n",
    "    return pd.DataFrame([df.to_numpy().T.flatten()])\n",
    "\n",
    "def get_k_components(pca, threshold=0.90):\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    k = np.argmax(cumulative_variance >= threshold) + 1\n",
    "    return k\n",
    "\n",
    "def reconstruct_from_pca(test_df, pca, k_components):\n",
    "    eigenvectors_k = pca.components_[:k_components, :]\n",
    "    projected = np.dot(test_df, eigenvectors_k.T)\n",
    "    reconstructed = np.dot(projected, eigenvectors_k)\n",
    "    return pd.DataFrame(reconstructed)\n",
    "\n",
    "def compute_errors(original, reconstructed):\n",
    "    original_np = original.to_numpy()\n",
    "    reconstructed_np = reconstructed.to_numpy()\n",
    "    ssd = np.sum((original_np - reconstructed_np) ** 2)\n",
    "    mse = mean_squared_error(original_np, reconstructed_np)\n",
    "    return ssd, mse\n",
    "\n",
    "\n",
    "# === Load and Process All Files ===\n",
    "csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "abhay_trials, arjun_trials = [], []\n",
    "\n",
    "for file in csv_files:\n",
    "    path = os.path.join(input_folder, file)\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        df = df.iloc[1:].reset_index(drop=True)\n",
    "        df = df.iloc[:, :39]\n",
    "\n",
    "        abhay_df = df.iloc[:, :19].copy()\n",
    "        arjun_df = df.iloc[:, 20:].copy()\n",
    "\n",
    "        abhay_df.columns = [f\"Abhay_{col}\" for col in expected_columns]\n",
    "        arjun_df.columns = [f\"Arjun_{col}\" for col in expected_columns]\n",
    "\n",
    "        abhay_df.dropna(axis=1, how='all', inplace=True)\n",
    "        arjun_df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "        abhay_t1, abhay_t2 = split_trials(abhay_df)\n",
    "        arjun_t1, arjun_t2 = split_trials(arjun_df)\n",
    "\n",
    "        abhay_trials.extend([abhay_t1, abhay_t2])\n",
    "        arjun_trials.extend([arjun_t1, arjun_t2])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# === Resample all trials ===\n",
    "all_lengths = [len(df) for df in abhay_trials + arjun_trials]\n",
    "median_len = int(np.median(all_lengths))\n",
    "print(f\"Using median trial length: {median_len}\")\n",
    "\n",
    "abhay_trials = safe_resample(abhay_trials, 'Abhay_time', median_len)\n",
    "arjun_trials = safe_resample(arjun_trials, 'Arjun_time', median_len)\n",
    "\n",
    "# === Cascade Each Trial Individually ===\n",
    "cascaded_abhay = pd.concat([cascade_single_trial(trial) for trial in abhay_trials], ignore_index=True)\n",
    "cascaded_arjun = pd.concat([cascade_single_trial(trial) for trial in arjun_trials], ignore_index=True)\n",
    "print(f\"Cascaded Abhay: {cascaded_abhay.shape}, Arjun: {cascaded_arjun.shape}\")\n",
    "\n",
    "# === Normalize ===\n",
    "scaler_abhay = MinMaxScaler()\n",
    "normalized_abhay = pd.DataFrame(scaler_abhay.fit_transform(cascaded_abhay))\n",
    "\n",
    "scaler_arjun = MinMaxScaler()\n",
    "normalized_arjun = pd.DataFrame(scaler_arjun.fit_transform(cascaded_arjun))\n",
    "\n",
    "# === Train/Test Split (70/30 per person) ===\n",
    "train_abhay = normalized_abhay.iloc[:140].reset_index(drop=True)\n",
    "test_abhay = normalized_abhay.iloc[140:].reset_index(drop=True)\n",
    "\n",
    "train_arjun = normalized_arjun.iloc[:140].reset_index(drop=True)\n",
    "test_arjun = normalized_arjun.iloc[140:].reset_index(drop=True)\n",
    "\n",
    "# === PCA Training ===\n",
    "pca_abhay = PCA().fit(train_abhay)\n",
    "pca_arjun = PCA().fit(train_arjun)\n",
    "\n",
    "# === Determine top-k for 90% variance ===\n",
    "k_abhay = get_k_components(pca_abhay, threshold=0.90)\n",
    "k_arjun = get_k_components(pca_arjun, threshold=0.90)\n",
    "print(f\"Abhay PCA components (90% var): {k_abhay}\")\n",
    "print(f\"Arjun PCA components (90% var): {k_arjun}\")\n",
    "\n",
    "# === Reconstruct test data ===\n",
    "reconstructed_abhay = reconstruct_from_pca(test_abhay, pca_abhay, k_abhay)\n",
    "reconstructed_arjun = reconstruct_from_pca(test_arjun, pca_arjun, k_arjun)\n",
    "\n",
    "# === Compute Errors ===\n",
    "ssd_abhay, mse_abhay = compute_errors(test_abhay, reconstructed_abhay)\n",
    "ssd_arjun, mse_arjun = compute_errors(test_arjun, reconstructed_arjun)\n",
    "\n",
    "# === Print Final Output ===\n",
    "print(\"\\nâœ… PCA & Reconstruction Complete\")\n",
    "print(f\"Abhay: Test {test_abhay.shape} -> Reconstructed {reconstructed_abhay.shape}\")\n",
    "print(f\"Arjun: Test {test_arjun.shape} -> Reconstructed {reconstructed_arjun.shape}\")\n",
    "print(f\"\\nðŸ“Š Abhay SSD: {ssd_abhay:.4f}, MSE: {mse_abhay:.6f}\")\n",
    "print(f\"ðŸ“Š Arjun SSD: {ssd_arjun:.4f}, MSE: {mse_arjun:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b4a10b-ea58-4c02-85cc-042e0d499196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
