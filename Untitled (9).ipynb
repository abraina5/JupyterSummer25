{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da121577-9775-4be2-af7c-63d033870639",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 126\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# === Get median length and resample ===\u001b[39;00m\n\u001b[0;32m    125\u001b[0m all_lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m all_abhay_trial1 \u001b[38;5;241m+\u001b[39m all_abhay_trial2 \u001b[38;5;241m+\u001b[39m all_arjun_trial1 \u001b[38;5;241m+\u001b[39m all_arjun_trial2]\n\u001b[1;32m--> 126\u001b[0m median_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmedian(all_lengths))\n\u001b[0;32m    128\u001b[0m all_abhay_trial1 \u001b[38;5;241m=\u001b[39m safe_resample(all_abhay_trial1, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbhay_time\u001b[39m\u001b[38;5;124m'\u001b[39m, median_len)\n\u001b[0;32m    129\u001b[0m all_abhay_trial2 \u001b[38;5;241m=\u001b[39m safe_resample(all_abhay_trial2, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbhay_time\u001b[39m\u001b[38;5;124m'\u001b[39m, median_len)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Constants ===\n",
    "expected_columns = [\n",
    "    'time', 'palm_position_x', 'palm_position_y', 'palm_position_z',\n",
    "    'palm_normal_x', 'palm_normal_y', 'palm_normal_z',\n",
    "    'palm_direction_x', 'palm_direction_y', 'palm_direction_z',\n",
    "    'hand_grab_angle', 'hand_grab_strength', 'hand_pinch_angle', 'hand_pinch_strength',\n",
    "    'thumb_extension', 'index_extension', 'middle_extension', 'ring_extension', 'pinky_extension'\n",
    "]\n",
    "TRAIN_ROWS = 70\n",
    "TEST_ROWS = 30\n",
    "input_folder = r\"C:\\Users\\Abhay\\Downloads\\ExportedSheets\"\n",
    "\n",
    "# === Helpers ===\n",
    "def is_row_empty(row):\n",
    "    return all(pd.isna(cell) or (isinstance(cell, str) and cell.strip() == '') for cell in row)\n",
    "\n",
    "def trim_leading_empty_rows(df):\n",
    "    for i in range(len(df)):\n",
    "        if not is_row_empty(df.iloc[i]):\n",
    "            return df.iloc[i:].reset_index(drop=True)\n",
    "    return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "def trim_trailing_empty_rows(df):\n",
    "    for i in reversed(range(len(df))):\n",
    "        if not is_row_empty(df.iloc[i]):\n",
    "            return df.iloc[:i+1].reset_index(drop=True)\n",
    "    return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "def find_trial_split_index(df):\n",
    "    for i in range(len(df)):\n",
    "        if is_row_empty(df.iloc[i]):\n",
    "            if i+1 < len(df) and is_row_empty(df.iloc[i+1]):\n",
    "                return i\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def split_trials(df):\n",
    "    df = trim_leading_empty_rows(df)\n",
    "    split_idx = find_trial_split_index(df)\n",
    "    if split_idx is None:\n",
    "        return df.reset_index(drop=True), pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    trial1 = df.iloc[:split_idx]\n",
    "    trial2 = df.iloc[split_idx+1:]\n",
    "\n",
    "    trial1 = trim_trailing_empty_rows(trial1)\n",
    "    trial2 = trim_leading_empty_rows(trial2)\n",
    "    trial2 = trim_trailing_empty_rows(trial2)\n",
    "\n",
    "    return trial1.reset_index(drop=True), trial2.reset_index(drop=True)\n",
    "\n",
    "def resample_by_time(df, time_col, target_rows):\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_numeric(df[time_col], errors='coerce')\n",
    "    df = df.dropna(subset=[time_col])\n",
    "    df = df.drop_duplicates(subset=time_col)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col != time_col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    df = df.set_index(time_col)\n",
    "    new_time_index = np.linspace(df.index.min(), df.index.max(), target_rows)\n",
    "    df_resampled = df.reindex(new_time_index)\n",
    "    df_resampled = df_resampled.interpolate(method='linear', axis=0).reset_index()\n",
    "    df_resampled.rename(columns={'index': time_col}, inplace=True)\n",
    "    return df_resampled\n",
    "\n",
    "def safe_resample(df_list, time_col, target_rows):\n",
    "    return [resample_by_time(df, time_col, target_rows)\n",
    "            for df in df_list if time_col in df.columns and not df.empty]\n",
    "\n",
    "def standardize_df(df, exclude_cols):\n",
    "    scaler = StandardScaler()\n",
    "    cols_to_scale = [col for col in df.columns if col not in exclude_cols]\n",
    "    df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "    return df\n",
    "\n",
    "def cascade(df, person_name):\n",
    "    columns_to_stack = [col for col in df.columns if col != f'{person_name}_time']\n",
    "    return pd.Series(np.concatenate([df[col].values for col in columns_to_stack]))\n",
    "\n",
    "def process_trials(trials, person_name):\n",
    "    train = [cascade(df.iloc[:TRAIN_ROWS], person_name) for df in trials]\n",
    "    test  = [cascade(df.iloc[TRAIN_ROWS:TRAIN_ROWS+TEST_ROWS], person_name) for df in trials]\n",
    "    return pd.DataFrame(train), pd.DataFrame(test)\n",
    "\n",
    "# === Read and process all CSVs ===\n",
    "csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv') and 'Hand Data Summer 25 (5)' in f]\n",
    "all_abhay_trial1, all_abhay_trial2 = [], []\n",
    "all_arjun_trial1, all_arjun_trial2 = [], []\n",
    "\n",
    "for file in csv_files:\n",
    "    path = os.path.join(input_folder, file)\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        df = df.iloc[1:].reset_index(drop=True)\n",
    "        df = df.iloc[:, :39]  # 19 + 1 blank + 19\n",
    "\n",
    "        abhay_df = df.iloc[:, :19].copy()\n",
    "        arjun_df = df.iloc[:, 20:].copy()\n",
    "\n",
    "        abhay_df.columns = [f\"Abhay_{col}\" for col in expected_columns]\n",
    "        arjun_df.columns = [f\"Arjun_{col}\" for col in expected_columns]\n",
    "\n",
    "        abhay_df.dropna(axis=1, how='all', inplace=True)\n",
    "        arjun_df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "        abhay_t1, abhay_t2 = split_trials(abhay_df)\n",
    "        arjun_t1, arjun_t2 = split_trials(arjun_df)\n",
    "\n",
    "        all_abhay_trial1.append(abhay_t1)\n",
    "        all_abhay_trial2.append(abhay_t2)\n",
    "        all_arjun_trial1.append(arjun_t1)\n",
    "        all_arjun_trial2.append(arjun_t2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# === Get median length and resample ===\n",
    "all_lengths = [len(df) for df in all_abhay_trial1 + all_abhay_trial2 + all_arjun_trial1 + all_arjun_trial2]\n",
    "median_len = int(np.median(all_lengths))\n",
    "\n",
    "all_abhay_trial1 = safe_resample(all_abhay_trial1, 'Abhay_time', median_len)\n",
    "all_abhay_trial2 = safe_resample(all_abhay_trial2, 'Abhay_time', median_len)\n",
    "all_arjun_trial1 = safe_resample(all_arjun_trial1, 'Arjun_time', median_len)\n",
    "all_arjun_trial2 = safe_resample(all_arjun_trial2, 'Arjun_time', median_len)\n",
    "\n",
    "all_abhay_trial1 = [standardize_df(df, ['Abhay_time']) for df in all_abhay_trial1]\n",
    "all_abhay_trial2 = [standardize_df(df, ['Abhay_time']) for df in all_abhay_trial2]\n",
    "all_arjun_trial1 = [standardize_df(df, ['Arjun_time']) for df in all_arjun_trial1]\n",
    "all_arjun_trial2 = [standardize_df(df, ['Arjun_time']) for df in all_arjun_trial2]\n",
    "\n",
    "# === Cascade into training/testing sets ===\n",
    "abhay_train_df1, abhay_test_df1 = process_trials(all_abhay_trial1, 'Abhay')\n",
    "abhay_train_df2, abhay_test_df2 = process_trials(all_abhay_trial2, 'Abhay')\n",
    "arjun_train_df1, arjun_test_df1 = process_trials(all_arjun_trial1, 'Arjun')\n",
    "arjun_train_df2, arjun_test_df2 = process_trials(all_arjun_trial2, 'Arjun')\n",
    "\n",
    "# === Final merge (optional, or you can keep separate) ===\n",
    "abhay_train_df = pd.concat([abhay_train_df1, abhay_train_df2], ignore_index=True)\n",
    "abhay_test_df = pd.concat([abhay_test_df1, abhay_test_df2], ignore_index=True)\n",
    "arjun_train_df = pd.concat([arjun_train_df1, arjun_train_df2], ignore_index=True)\n",
    "arjun_test_df = pd.concat([arjun_test_df1, arjun_test_df2], ignore_index=True)\n",
    "\n",
    "# === Optional Save ===\n",
    "# abhay_train_df.to_csv(\"abhay_train_cascaded.csv\", index=False)\n",
    "# abhay_test_df.to_csv(\"abhay_test_cascaded.csv\", index=False)\n",
    "# arjun_train_df.to_csv(\"arjun_train_cascaded.csv\", index=False)\n",
    "# arjun_test_df.to_csv(\"arjun_test_cascaded.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a557efe-259c-4634-9585-d1876b7c348d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Trial lengths from sheet: 0 ===\n",
      "Abhay Trial 1: 139 rows\n",
      "Abhay Trial 2: 130 rows\n",
      "Arjun Trial 1: 129 rows\n",
      "Arjun Trial 2: 143 rows\n",
      "Using median trial length: 134\n",
      "✅ Cascading complete.\n",
      "\n",
      "=== Final Cascaded Dimensions ===\n",
      "Abhay Training Set: (2, 1260)\n",
      "Abhay Testing Set : (2, 540)\n",
      "Arjun Training Set: (2, 1260)\n",
      "Arjun Testing Set : (2, 540)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "os.path.exists(r\"C:\\Users\\Abhay\\Downloads\\Hand Data Summer 25 (5).xlsx\")\n",
    "\n",
    "\n",
    "# === Setup ===\n",
    "input_file = r\"C:\\Users\\Abhay\\Downloads\\Hand Data Summer 25 (5).xlsx\"\n",
    "sheet_name = 0  # Change to sheet name or index as needed (e.g., 'ASL - A' or 0)\n",
    "\n",
    "expected_columns = [\n",
    "    'time', 'palm_position_x', 'palm_position_y', 'palm_position_z',\n",
    "    'palm_normal_x', 'palm_normal_y', 'palm_normal_z',\n",
    "    'palm_direction_x', 'palm_direction_y', 'palm_direction_z',\n",
    "    'hand_grab_angle', 'hand_grab_strength', 'hand_pinch_angle', 'hand_pinch_strength',\n",
    "    'thumb_extension', 'index_extension', 'middle_extension', 'ring_extension', 'pinky_extension'\n",
    "]\n",
    "\n",
    "def is_row_empty(row):\n",
    "    return all(pd.isna(cell) or (isinstance(cell, str) and cell.strip() == '') for cell in row)\n",
    "\n",
    "def trim_leading_empty_rows(df):\n",
    "    for i in range(len(df)):\n",
    "        if not is_row_empty(df.iloc[i]):\n",
    "            return df.iloc[i:].reset_index(drop=True)\n",
    "    return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "def trim_trailing_empty_rows(df):\n",
    "    for i in reversed(range(len(df))):\n",
    "        if not is_row_empty(df.iloc[i]):\n",
    "            return df.iloc[:i+1].reset_index(drop=True)\n",
    "    return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "def find_trial_split_index(df):\n",
    "    for i in range(len(df)):\n",
    "        if is_row_empty(df.iloc[i]):\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def split_trials(df):\n",
    "    df = trim_leading_empty_rows(df)\n",
    "    split_idx = find_trial_split_index(df)\n",
    "    if split_idx is None:\n",
    "        return df.reset_index(drop=True), pd.DataFrame(columns=df.columns)\n",
    "    trial1 = df.iloc[:split_idx]\n",
    "    trial2 = df.iloc[split_idx+1:]\n",
    "    trial1 = trim_trailing_empty_rows(trial1)\n",
    "    trial2 = trim_leading_empty_rows(trial2)\n",
    "    trial2 = trim_trailing_empty_rows(trial2)\n",
    "    return trial1.reset_index(drop=True), trial2.reset_index(drop=True)\n",
    "\n",
    "def resample_by_time(df, time_col, target_rows):\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_numeric(df[time_col], errors='coerce')\n",
    "    df = df.dropna(subset=[time_col])\n",
    "    df = df.drop_duplicates(subset=time_col)\n",
    "    for col in df.columns:\n",
    "        if col != time_col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df = df.set_index(time_col)\n",
    "    new_time_index = np.linspace(df.index.min(), df.index.max(), target_rows)\n",
    "    df_resampled = df.reindex(new_time_index)\n",
    "    df_resampled = df_resampled.interpolate(method='linear', axis=0).reset_index()\n",
    "    df_resampled.rename(columns={'index': time_col}, inplace=True)\n",
    "    return df_resampled\n",
    "\n",
    "def standardize_df(df, exclude_cols):\n",
    "    scaler = StandardScaler()\n",
    "    cols_to_scale = [col for col in df.columns if col not in exclude_cols]\n",
    "    df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "    return df\n",
    "\n",
    "def cascade(df, person_name, start_row, end_row):\n",
    "    cols = [col for col in df.columns if col != f'{person_name}_time']\n",
    "    return pd.Series(np.concatenate([df.loc[start_row:end_row-1, col].values for col in cols]))\n",
    "\n",
    "# === Load and prepare ===\n",
    "df = pd.read_excel(input_file, sheet_name=sheet_name, header=None)\n",
    "df = df.iloc[1:].reset_index(drop=True)\n",
    "df = df.iloc[:, :39]  # 19 + 1 blank + 19\n",
    "\n",
    "# Split into Abhay and Arjun\n",
    "abhay_df = df.iloc[:, :19].copy()\n",
    "arjun_df = df.iloc[:, 20:].copy()\n",
    "abhay_df.columns = [f\"Abhay_{col}\" for col in expected_columns]\n",
    "arjun_df.columns = [f\"Arjun_{col}\" for col in expected_columns]\n",
    "\n",
    "abhay_df.dropna(axis=1, how='all', inplace=True)\n",
    "arjun_df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# Split trials\n",
    "abhay_t1, abhay_t2 = split_trials(abhay_df)\n",
    "arjun_t1, arjun_t2 = split_trials(arjun_df)\n",
    "\n",
    "print(f\"\\n=== Trial lengths from sheet: {sheet_name} ===\")\n",
    "print(f\"Abhay Trial 1: {len(abhay_t1)} rows\")\n",
    "print(f\"Abhay Trial 2: {len(abhay_t2)} rows\")\n",
    "print(f\"Arjun Trial 1: {len(arjun_t1)} rows\")\n",
    "print(f\"Arjun Trial 2: {len(arjun_t2)} rows\")\n",
    "\n",
    "# Median length\n",
    "lengths = [len(d) for d in [abhay_t1, abhay_t2, arjun_t1, arjun_t2] if not d.empty]\n",
    "if not lengths:\n",
    "    raise ValueError(\"No usable trials found\")\n",
    "median_len = int(np.median(lengths))\n",
    "print(f\"Using median trial length: {median_len}\")\n",
    "\n",
    "# Resample\n",
    "abhay_t1 = resample_by_time(abhay_t1, 'Abhay_time', median_len)\n",
    "abhay_t2 = resample_by_time(abhay_t2, 'Abhay_time', median_len)\n",
    "arjun_t1 = resample_by_time(arjun_t1, 'Arjun_time', median_len)\n",
    "arjun_t2 = resample_by_time(arjun_t2, 'Arjun_time', median_len)\n",
    "\n",
    "# Standardize\n",
    "abhay_t1 = standardize_df(abhay_t1, ['Abhay_time'])\n",
    "abhay_t2 = standardize_df(abhay_t2, ['Abhay_time'])\n",
    "arjun_t1 = standardize_df(arjun_t1, ['Arjun_time'])\n",
    "arjun_t2 = standardize_df(arjun_t2, ['Arjun_time'])\n",
    "\n",
    "# Cascade\n",
    "TRAIN_ROWS = 70\n",
    "TEST_ROWS = 30\n",
    "\n",
    "abhay_train_df = pd.DataFrame([\n",
    "    cascade(abhay_t1, 'Abhay', 0, TRAIN_ROWS),\n",
    "    cascade(abhay_t2, 'Abhay', 0, TRAIN_ROWS)\n",
    "])\n",
    "abhay_test_df = pd.DataFrame([\n",
    "    cascade(abhay_t1, 'Abhay', TRAIN_ROWS, TRAIN_ROWS + TEST_ROWS),\n",
    "    cascade(abhay_t2, 'Abhay', TRAIN_ROWS, TRAIN_ROWS + TEST_ROWS)\n",
    "])\n",
    "arjun_train_df = pd.DataFrame([\n",
    "    cascade(arjun_t1, 'Arjun', 0, TRAIN_ROWS),\n",
    "    cascade(arjun_t2, 'Arjun', 0, TRAIN_ROWS)\n",
    "])\n",
    "arjun_test_df = pd.DataFrame([\n",
    "    cascade(arjun_t1, 'Arjun', TRAIN_ROWS, TRAIN_ROWS + TEST_ROWS),\n",
    "    cascade(arjun_t2, 'Arjun', TRAIN_ROWS, TRAIN_ROWS + TEST_ROWS)\n",
    "])\n",
    "\n",
    "# === OPTIONAL: Save to file\n",
    "# abhay_train_df.to_csv(\"abhay_train.csv\", index=False)\n",
    "# abhay_test_df.to_csv(\"abhay_test.csv\", index=False)\n",
    "# arjun_train_df.to_csv(\"arjun_train.csv\", index=False)\n",
    "# arjun_test_df.to_csv(\"arjun_test.csv\", index=False)\n",
    "\n",
    "print(\"✅ Cascading complete.\")\n",
    "print(\"\\n=== Final Cascaded Dimensions ===\")\n",
    "print(f\"Abhay Training Set: {abhay_train_df.shape}\")\n",
    "print(f\"Abhay Testing Set : {abhay_test_df.shape}\")\n",
    "print(f\"Arjun Training Set: {arjun_train_df.shape}\")\n",
    "print(f\"Arjun Testing Set : {arjun_test_df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794391e-a8f6-49f6-b5ec-361a3d82ff04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using median trial length: 118\n",
      "Air Quotes.csv:\n",
      "  Abhay Trial 1: 63 rows\n",
      "  Abhay Trial 2: 96 rows\n",
      "  Arjun Trial 1: 112 rows\n",
      "  Arjun Trial 2: 112 rows\n",
      "ASL - 1.csv:\n",
      "  Abhay Trial 1: 132 rows\n",
      "  Abhay Trial 2: 120 rows\n",
      "  Arjun Trial 1: 138 rows\n",
      "  Arjun Trial 2: 142 rows\n",
      "ASL - 10.csv:\n",
      "  Abhay Trial 1: 139 rows\n",
      "  Abhay Trial 2: 126 rows\n",
      "  Arjun Trial 1: 135 rows\n",
      "  Arjun Trial 2: 106 rows\n",
      "ASL - 2.csv:\n",
      "  Abhay Trial 1: 134 rows\n",
      "  Abhay Trial 2: 141 rows\n",
      "  Arjun Trial 1: 145 rows\n",
      "  Arjun Trial 2: 136 rows\n",
      "ASL - 3.csv:\n",
      "  Abhay Trial 1: 102 rows\n",
      "  Abhay Trial 2: 128 rows\n",
      "  Arjun Trial 1: 74 rows\n",
      "  Arjun Trial 2: 106 rows\n",
      "ASL - 4.csv:\n",
      "  Abhay Trial 1: 124 rows\n",
      "  Abhay Trial 2: 105 rows\n",
      "  Arjun Trial 1: 118 rows\n",
      "  Arjun Trial 2: 105 rows\n",
      "ASL - 5.csv:\n",
      "  Abhay Trial 1: 107 rows\n",
      "  Abhay Trial 2: 106 rows\n",
      "  Arjun Trial 1: 102 rows\n",
      "  Arjun Trial 2: 98 rows\n",
      "ASL - 6.csv:\n",
      "  Abhay Trial 1: 105 rows\n",
      "  Abhay Trial 2: 105 rows\n",
      "  Arjun Trial 1: 88 rows\n",
      "  Arjun Trial 2: 107 rows\n",
      "ASL - 7.csv:\n",
      "  Abhay Trial 1: 118 rows\n",
      "  Abhay Trial 2: 108 rows\n",
      "  Arjun Trial 1: 123 rows\n",
      "  Arjun Trial 2: 126 rows\n",
      "ASL - 8.csv:\n",
      "  Abhay Trial 1: 138 rows\n",
      "  Abhay Trial 2: 132 rows\n",
      "  Arjun Trial 1: 130 rows\n",
      "  Arjun Trial 2: 126 rows\n",
      "ASL - 9.csv:\n",
      "  Abhay Trial 1: 123 rows\n",
      "  Abhay Trial 2: 116 rows\n",
      "  Arjun Trial 1: 160 rows\n",
      "  Arjun Trial 2: 130 rows\n",
      "ASL - A.csv:\n",
      "  Abhay Trial 1: 139 rows\n",
      "  Abhay Trial 2: 130 rows\n",
      "  Arjun Trial 1: 129 rows\n",
      "  Arjun Trial 2: 143 rows\n",
      "ASL - B.csv:\n",
      "  Abhay Trial 1: 138 rows\n",
      "  Abhay Trial 2: 138 rows\n",
      "  Arjun Trial 1: 153 rows\n",
      "  Arjun Trial 2: 142 rows\n",
      "ASL - Bad.csv:\n",
      "  Abhay Trial 1: 99 rows\n",
      "  Abhay Trial 2: 75 rows\n",
      "  Arjun Trial 1: 115 rows\n",
      "  Arjun Trial 2: 127 rows\n",
      "ASL - C.csv:\n",
      "  Abhay Trial 1: 141 rows\n",
      "  Abhay Trial 2: 143 rows\n",
      "  Arjun Trial 1: 136 rows\n",
      "  Arjun Trial 2: 135 rows\n",
      "ASL - D.csv:\n",
      "  Abhay Trial 1: 149 rows\n",
      "  Abhay Trial 2: 134 rows\n",
      "  Arjun Trial 1: 140 rows\n",
      "  Arjun Trial 2: 144 rows\n",
      "ASL - Drink.csv:\n",
      "  Abhay Trial 1: 125 rows\n",
      "  Abhay Trial 2: 131 rows\n",
      "  Arjun Trial 1: 115 rows\n",
      "  Arjun Trial 2: 130 rows\n",
      "ASL - E.csv:\n",
      "  Abhay Trial 1: 155 rows\n",
      "  Abhay Trial 2: 124 rows\n",
      "  Arjun Trial 1: 135 rows\n",
      "  Arjun Trial 2: 133 rows\n",
      "ASL - Eat.csv:\n",
      "  Abhay Trial 1: 127 rows\n",
      "  Abhay Trial 2: 144 rows\n",
      "  Arjun Trial 1: 130 rows\n",
      "  Arjun Trial 2: 134 rows\n",
      "ASL - F.csv:\n",
      "  Abhay Trial 1: 157 rows\n",
      "  Abhay Trial 2: 134 rows\n",
      "  Arjun Trial 1: 141 rows\n",
      "  Arjun Trial 2: 141 rows\n",
      "ASL - G.csv:\n",
      "  Abhay Trial 1: 128 rows\n",
      "  Abhay Trial 2: 141 rows\n",
      "  Arjun Trial 1: 136 rows\n",
      "  Arjun Trial 2: 141 rows\n",
      "ASL - Goodbye.csv:\n",
      "  Abhay Trial 1: 134 rows\n",
      "  Abhay Trial 2: 134 rows\n",
      "  Arjun Trial 1: 131 rows\n",
      "  Arjun Trial 2: 134 rows\n",
      "ASL - Hello.csv:\n",
      "  Abhay Trial 1: 102 rows\n",
      "  Abhay Trial 2: 90 rows\n",
      "  Arjun Trial 1: 100 rows\n",
      "  Arjun Trial 2: 103 rows\n",
      "ASL - I love you.csv:\n",
      "  Abhay Trial 1: 142 rows\n",
      "  Abhay Trial 2: 130 rows\n",
      "  Arjun Trial 1: 133 rows\n",
      "  Arjun Trial 2: 137 rows\n",
      "ASL - I.csv:\n",
      "  Abhay Trial 1: 118 rows\n",
      "  Abhay Trial 2: 103 rows\n",
      "  Arjun Trial 1: 145 rows\n",
      "  Arjun Trial 2: 100 rows\n",
      "ASL - J.csv:\n",
      "  Abhay Trial 1: 121 rows\n",
      "  Abhay Trial 2: 123 rows\n",
      "  Arjun Trial 1: 110 rows\n",
      "  Arjun Trial 2: 94 rows\n",
      "ASL - K.csv:\n",
      "  Abhay Trial 1: 113 rows\n",
      "  Abhay Trial 2: 114 rows\n",
      "  Arjun Trial 1: 111 rows\n",
      "  Arjun Trial 2: 116 rows\n",
      "ASL - L.csv:\n",
      "  Abhay Trial 1: 108 rows\n",
      "  Abhay Trial 2: 108 rows\n",
      "  Arjun Trial 1: 108 rows\n",
      "  Arjun Trial 2: 108 rows\n",
      "ASL - M.csv:\n",
      "  Abhay Trial 1: 110 rows\n",
      "  Abhay Trial 2: 111 rows\n",
      "  Arjun Trial 1: 111 rows\n",
      "  Arjun Trial 2: 103 rows\n",
      "ASL - Me.csv:\n",
      "  Abhay Trial 1: 114 rows\n",
      "  Abhay Trial 2: 134 rows\n",
      "  Arjun Trial 1: 136 rows\n",
      "  Arjun Trial 2: 126 rows\n",
      "ASL - N.csv:\n",
      "  Abhay Trial 1: 109 rows\n",
      "  Abhay Trial 2: 99 rows\n",
      "  Arjun Trial 1: 118 rows\n",
      "  Arjun Trial 2: 110 rows\n",
      "ASL - No.csv:\n",
      "  Abhay Trial 1: 157 rows\n",
      "  Abhay Trial 2: 91 rows\n",
      "  Arjun Trial 1: 85 rows\n",
      "  Arjun Trial 2: 96 rows\n",
      "ASL - O.csv:\n",
      "  Abhay Trial 1: 118 rows\n",
      "  Abhay Trial 2: 116 rows\n",
      "  Arjun Trial 1: 116 rows\n",
      "  Arjun Trial 2: 111 rows\n",
      "ASL - Q.csv:\n",
      "  Abhay Trial 1: 111 rows\n",
      "  Abhay Trial 2: 99 rows\n",
      "  Arjun Trial 1: 111 rows\n",
      "  Arjun Trial 2: 113 rows\n",
      "ASL - R.csv:\n",
      "  Abhay Trial 1: 135 rows\n",
      "  Abhay Trial 2: 96 rows\n",
      "  Arjun Trial 1: 102 rows\n",
      "  Arjun Trial 2: 132 rows\n",
      "ASL - S.csv:\n",
      "  Abhay Trial 1: 119 rows\n",
      "  Abhay Trial 2: 110 rows\n",
      "  Arjun Trial 1: 108 rows\n",
      "  Arjun Trial 2: 126 rows\n",
      "ASL - T.csv:\n",
      "  Abhay Trial 1: 117 rows\n",
      "  Abhay Trial 2: 120 rows\n",
      "  Arjun Trial 1: 118 rows\n",
      "  Arjun Trial 2: 120 rows\n",
      "ASL - Thank you.csv:\n",
      "  Abhay Trial 1: 115 rows\n",
      "  Abhay Trial 2: 96 rows\n",
      "  Arjun Trial 1: 108 rows\n",
      "  Arjun Trial 2: 124 rows\n",
      "ASL - Tomorrow.csv:\n",
      "  Abhay Trial 1: 135 rows\n",
      "  Abhay Trial 2: 131 rows\n",
      "  Arjun Trial 1: 131 rows\n",
      "  Arjun Trial 2: 134 rows\n",
      "ASL - U.csv:\n",
      "  Abhay Trial 1: 106 rows\n",
      "  Abhay Trial 2: 109 rows\n",
      "  Arjun Trial 1: 129 rows\n",
      "  Arjun Trial 2: 112 rows\n",
      "ASL - V.csv:\n",
      "  Abhay Trial 1: 115 rows\n",
      "  Abhay Trial 2: 116 rows\n",
      "  Arjun Trial 1: 119 rows\n",
      "  Arjun Trial 2: 118 rows\n",
      "ASL - W.csv:\n",
      "  Abhay Trial 1: 115 rows\n",
      "  Abhay Trial 2: 112 rows\n",
      "  Arjun Trial 1: 112 rows\n",
      "  Arjun Trial 2: 108 rows\n",
      "ASL - X.csv:\n",
      "  Abhay Trial 1: 135 rows\n",
      "  Abhay Trial 2: 118 rows\n",
      "  Arjun Trial 1: 99 rows\n",
      "  Arjun Trial 2: 95 rows\n",
      "ASL - Y.csv:\n",
      "  Abhay Trial 1: 116 rows\n",
      "  Abhay Trial 2: 102 rows\n",
      "  Arjun Trial 1: 111 rows\n",
      "  Arjun Trial 2: 112 rows\n",
      "ASL - Yes.csv:\n",
      "  Abhay Trial 1: 89 rows\n",
      "  Abhay Trial 2: 110 rows\n",
      "  Arjun Trial 1: 99 rows\n",
      "  Arjun Trial 2: 105 rows\n",
      "ASL - Z.csv:\n",
      "  Abhay Trial 1: 113 rows\n",
      "  Abhay Trial 2: 112 rows\n",
      "  Arjun Trial 1: 107 rows\n",
      "  Arjun Trial 2: 103 rows\n",
      "Coin Flip.csv:\n",
      "  Abhay Trial 1: 129 rows\n",
      "  Abhay Trial 2: 135 rows\n",
      "  Arjun Trial 1: 146 rows\n",
      "  Arjun Trial 2: 132 rows\n",
      "Computer Charger.csv:\n",
      "  Abhay Trial 1: 145 rows\n",
      "  Abhay Trial 2: 138 rows\n",
      "  Arjun Trial 1: 120 rows\n",
      "  Arjun Trial 2: 111 rows\n",
      "Fingers Crossed.csv:\n",
      "  Abhay Trial 1: 113 rows\n",
      "  Abhay Trial 2: 128 rows\n",
      "  Arjun Trial 1: 139 rows\n",
      "  Arjun Trial 2: 139 rows\n",
      "Fist.csv:\n",
      "  Abhay Trial 1: 148 rows\n",
      "  Abhay Trial 2: 135 rows\n",
      "  Arjun Trial 1: 123 rows\n",
      "  Arjun Trial 2: 142 rows\n",
      "Flicking.csv:\n",
      "  Abhay Trial 1: 87 rows\n",
      "  Abhay Trial 2: 100 rows\n",
      "  Arjun Trial 1: 101 rows\n",
      "  Arjun Trial 2: 89 rows\n",
      "Frisbee.csv:\n",
      "  Abhay Trial 1: 140 rows\n",
      "  Abhay Trial 2: 109 rows\n",
      "  Arjun Trial 1: 107 rows\n",
      "  Arjun Trial 2: 86 rows\n",
      "Grabbing a comb.csv:\n",
      "  Abhay Trial 1: 109 rows\n",
      "  Abhay Trial 2: 130 rows\n",
      "  Arjun Trial 1: 130 rows\n",
      "  Arjun Trial 2: 132 rows\n",
      "Grabbing a flag.csv:\n",
      "  Abhay Trial 1: 137 rows\n",
      "  Abhay Trial 2: 115 rows\n",
      "  Arjun Trial 1: 139 rows\n",
      "  Arjun Trial 2: 111 rows\n",
      "Grabbing a marker.csv:\n",
      "  Abhay Trial 1: 116 rows\n",
      "  Abhay Trial 2: 113 rows\n",
      "  Arjun Trial 1: 105 rows\n",
      "  Arjun Trial 2: 124 rows\n",
      "Grabbing Napkin.csv:\n",
      "  Abhay Trial 1: 126 rows\n",
      "  Abhay Trial 2: 92 rows\n",
      "  Arjun Trial 1: 125 rows\n",
      "  Arjun Trial 2: 125 rows\n",
      "Grabbing Pen.csv:\n",
      "  Abhay Trial 1: 124 rows\n",
      "  Abhay Trial 2: 124 rows\n",
      "  Arjun Trial 1: 113 rows\n",
      "  Arjun Trial 2: 121 rows\n",
      "Grabbing Pencil.csv:\n",
      "  Abhay Trial 1: 101 rows\n",
      "  Abhay Trial 2: 132 rows\n",
      "  Arjun Trial 1: 120 rows\n",
      "  Arjun Trial 2: 129 rows\n",
      "Grabbing toothbrush.csv:\n",
      "  Abhay Trial 1: 137 rows\n",
      "  Abhay Trial 2: 109 rows\n",
      "  Arjun Trial 1: 144 rows\n",
      "  Arjun Trial 2: 123 rows\n",
      "Hourglass.csv:\n",
      "  Abhay Trial 1: 105 rows\n",
      "  Abhay Trial 2: 100 rows\n",
      "  Arjun Trial 1: 107 rows\n",
      "  Arjun Trial 2: 112 rows\n",
      "Lemon Squeezer.csv:\n",
      "  Abhay Trial 1: 115 rows\n",
      "  Abhay Trial 2: 105 rows\n",
      "  Arjun Trial 1: 112 rows\n",
      "  Arjun Trial 2: 102 rows\n",
      "Looking at watch.csv:\n",
      "  Abhay Trial 1: 107 rows\n",
      "  Abhay Trial 2: 160 rows\n",
      "  Arjun Trial 1: 106 rows\n",
      "  Arjun Trial 2: 106 rows\n",
      "Middle finger.csv:\n",
      "  Abhay Trial 1: 106 rows\n",
      "  Abhay Trial 2: 154 rows\n",
      "  Arjun Trial 1: 139 rows\n",
      "  Arjun Trial 2: 137 rows\n",
      "Opening a book cover.csv:\n",
      "  Abhay Trial 1: 115 rows\n",
      "  Abhay Trial 2: 105 rows\n",
      "  Arjun Trial 1: 104 rows\n",
      "  Arjun Trial 2: 89 rows\n",
      "Opening lipstick.csv:\n",
      "  Abhay Trial 1: 147 rows\n",
      "  Abhay Trial 2: 146 rows\n",
      "  Arjun Trial 1: 109 rows\n",
      "  Arjun Trial 2: 132 rows\n",
      "Opening Plastic Bottle.csv:\n",
      "  Abhay Trial 1: 145 rows\n",
      "  Abhay Trial 2: 133 rows\n",
      "  Arjun Trial 1: 132 rows\n",
      "  Arjun Trial 2: 106 rows\n",
      "Phone Charger.csv:\n",
      "  Abhay Trial 1: 141 rows\n",
      "  Abhay Trial 2: 125 rows\n",
      "  Arjun Trial 1: 127 rows\n",
      "  Arjun Trial 2: 128 rows\n",
      "Picking and using nail clipper.csv:\n",
      "  Abhay Trial 1: 108 rows\n",
      "  Abhay Trial 2: 109 rows\n",
      "  Arjun Trial 1: 89 rows\n",
      "  Arjun Trial 2: 105 rows\n",
      "Picking and using scissors.csv:\n",
      "  Abhay Trial 1: 115 rows\n",
      "  Abhay Trial 2: 108 rows\n",
      "  Arjun Trial 1: 129 rows\n",
      "  Arjun Trial 2: 104 rows\n",
      "Picking card from deck.csv:\n",
      "  Abhay Trial 1: 103 rows\n",
      "  Abhay Trial 2: 120 rows\n",
      "  Arjun Trial 1: 134 rows\n",
      "  Arjun Trial 2: 128 rows\n",
      "Picking up a banana.csv:\n",
      "  Abhay Trial 1: 100 rows\n",
      "  Abhay Trial 2: 110 rows\n",
      "  Arjun Trial 1: 103 rows\n",
      "  Arjun Trial 2: 102 rows\n",
      "Picking up a fidget spinner.csv:\n",
      "  Abhay Trial 1: 137 rows\n",
      "  Abhay Trial 2: 125 rows\n",
      "  Arjun Trial 1: 125 rows\n",
      "  Arjun Trial 2: 124 rows\n",
      "Picking up a fork.csv:\n",
      "  Abhay Trial 1: 81 rows\n",
      "  Abhay Trial 2: 78 rows\n",
      "  Arjun Trial 1: 82 rows\n",
      "  Arjun Trial 2: 76 rows\n",
      "Picking up a knife.csv:\n",
      "  Abhay Trial 1: 154 rows\n",
      "  Abhay Trial 2: 156 rows\n",
      "  Arjun Trial 1: 128 rows\n",
      "  Arjun Trial 2: 151 rows\n",
      "Picking up a mug.csv:\n",
      "  Abhay Trial 1: 104 rows\n",
      "  Abhay Trial 2: 104 rows\n",
      "  Arjun Trial 1: 98 rows\n",
      "  Arjun Trial 2: 121 rows\n",
      "Picking up a sponn.csv:\n",
      "  Abhay Trial 1: 129 rows\n",
      "  Abhay Trial 2: 90 rows\n",
      "  Arjun Trial 1: 92 rows\n",
      "  Arjun Trial 2: 89 rows\n",
      "Picking up a tablet.csv:\n",
      "  Abhay Trial 1: 109 rows\n",
      "  Abhay Trial 2: 125 rows\n",
      "  Arjun Trial 1: 130 rows\n",
      "  Arjun Trial 2: 123 rows\n",
      "Picking up car key.csv:\n",
      "  Abhay Trial 1: 110 rows\n",
      "  Abhay Trial 2: 128 rows\n",
      "  Arjun Trial 1: 119 rows\n",
      "  Arjun Trial 2: 150 rows\n",
      "Picking up chapstick.csv:\n",
      "  Abhay Trial 1: 110 rows\n",
      "  Abhay Trial 2: 114 rows\n",
      "  Arjun Trial 1: 111 rows\n",
      "  Arjun Trial 2: 104 rows\n",
      "Picking up flashlight.csv:\n",
      "  Abhay Trial 1: 115 rows\n",
      "  Abhay Trial 2: 121 rows\n",
      "  Arjun Trial 1: 98 rows\n",
      "  Arjun Trial 2: 112 rows\n",
      "Picking up flyswatter.csv:\n",
      "  Abhay Trial 1: 132 rows\n",
      "  Abhay Trial 2: 128 rows\n",
      "  Arjun Trial 1: 138 rows\n",
      "  Arjun Trial 2: 133 rows\n",
      "Picking up screwdriver.csv:\n",
      "  Abhay Trial 1: 115 rows\n",
      "  Abhay Trial 2: 136 rows\n",
      "  Arjun Trial 1: 57 rows\n",
      "  Arjun Trial 2: 144 rows\n",
      "Picking up TV remote.csv:\n",
      "  Abhay Trial 1: 104 rows\n",
      "  Abhay Trial 2: 120 rows\n",
      "  Arjun Trial 1: 89 rows\n",
      "  Arjun Trial 2: 97 rows\n",
      "Pulling serran wrap.csv:\n",
      "  Abhay Trial 1: 128 rows\n",
      "  Abhay Trial 2: 128 rows\n",
      "  Arjun Trial 1: 124 rows\n",
      "  Arjun Trial 2: 133 rows\n",
      "Rolling a dice.csv:\n",
      "  Abhay Trial 1: 168 rows\n",
      "  Abhay Trial 2: 109 rows\n",
      "  Arjun Trial 1: 105 rows\n",
      "  Arjun Trial 2: 105 rows\n",
      "Rolling.csv:\n",
      "  Abhay Trial 1: 133 rows\n",
      "  Abhay Trial 2: 129 rows\n",
      "  Arjun Trial 1: 134 rows\n",
      "  Arjun Trial 2: 131 rows\n",
      "Salute.csv:\n",
      "  Abhay Trial 1: 75 rows\n",
      "  Abhay Trial 2: 100 rows\n",
      "  Arjun Trial 1: 79 rows\n",
      "  Arjun Trial 2: 82 rows\n",
      "Scisssor motion.csv:\n",
      "  Abhay Trial 1: 152 rows\n",
      "  Abhay Trial 2: 107 rows\n",
      "  Arjun Trial 1: 135 rows\n",
      "  Arjun Trial 2: 115 rows\n",
      "Snapping.csv:\n",
      "  Abhay Trial 1: 113 rows\n",
      "  Abhay Trial 2: 121 rows\n",
      "  Arjun Trial 1: 113 rows\n",
      "  Arjun Trial 2: 105 rows\n",
      "So-so.csv:\n",
      "  Abhay Trial 1: 106 rows\n",
      "  Abhay Trial 2: 96 rows\n",
      "  Arjun Trial 1: 89 rows\n",
      "  Arjun Trial 2: 96 rows\n",
      "Thumbs up.csv:\n",
      "  Abhay Trial 1: 86 rows\n",
      "  Abhay Trial 2: 89 rows\n",
      "  Arjun Trial 1: 89 rows\n",
      "  Arjun Trial 2: 81 rows\n",
      "Uncapping toothpaste.csv:\n",
      "  Abhay Trial 1: 92 rows\n",
      "  Abhay Trial 2: 95 rows\n",
      "  Arjun Trial 1: 92 rows\n",
      "  Arjun Trial 2: 77 rows\n",
      "Using a key.csv:\n",
      "  Abhay Trial 1: 130 rows\n",
      "  Abhay Trial 2: 134 rows\n",
      "  Arjun Trial 1: 139 rows\n",
      "  Arjun Trial 2: 141 rows\n",
      "Using clothespin.csv:\n",
      "  Abhay Trial 1: 132 rows\n",
      "  Abhay Trial 2: 111 rows\n",
      "  Arjun Trial 1: 127 rows\n",
      "  Arjun Trial 2: 101 rows\n",
      "Using deoderant.csv:\n",
      "  Abhay Trial 1: 138 rows\n",
      "  Abhay Trial 2: 127 rows\n",
      "  Arjun Trial 1: 105 rows\n",
      "  Arjun Trial 2: 110 rows\n",
      "Using hand sanitizer.csv:\n",
      "  Abhay Trial 1: 107 rows\n",
      "  Abhay Trial 2: 109 rows\n",
      "  Arjun Trial 1: 134 rows\n",
      "  Arjun Trial 2: 108 rows\n",
      "Using hose.csv:\n",
      "  Abhay Trial 1: 122 rows\n",
      "  Abhay Trial 2: 142 rows\n",
      "  Arjun Trial 1: 126 rows\n",
      "  Arjun Trial 2: 128 rows\n",
      "Using Tongs.csv:\n",
      "  Abhay Trial 1: 141 rows\n",
      "  Abhay Trial 2: 138 rows\n",
      "  Arjun Trial 1: 134 rows\n",
      "  Arjun Trial 2: 134 rows\n",
      "Waving.csv:\n",
      "  Abhay Trial 1: 115 rows\n",
      "  Abhay Trial 2: 116 rows\n",
      "  Arjun Trial 1: 95 rows\n",
      "  Arjun Trial 2: 105 rows\n",
      "Wrench screwing.csv:\n",
      "  Abhay Trial 1: 120 rows\n",
      "  Abhay Trial 2: 120 rows\n",
      "  Arjun Trial 1: 129 rows\n",
      "  Arjun Trial 2: 123 rows\n",
      "\n",
      "=== Trial Lengths Per File ===\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Column Definition ===\n",
    "expected_columns = [\n",
    "    'time', 'palm_position_x', 'palm_position_y', 'palm_position_z',\n",
    "    'palm_normal_x', 'palm_normal_y', 'palm_normal_z',\n",
    "    'palm_direction_x', 'palm_direction_y', 'palm_direction_z',\n",
    "    'hand_grab_angle', 'hand_grab_strength', 'hand_pinch_angle', 'hand_pinch_strength',\n",
    "    'thumb_extension', 'index_extension', 'middle_extension', 'ring_extension', 'pinky_extension'\n",
    "]\n",
    "\n",
    "# === Utilities ===\n",
    "def is_row_empty(row):\n",
    "    return all(pd.isna(cell) or (isinstance(cell, str) and cell.strip() == '') for cell in row)\n",
    "\n",
    "def trim_leading_empty_rows(df):\n",
    "    for i in range(len(df)):\n",
    "        if not is_row_empty(df.iloc[i]):\n",
    "            return df.iloc[i:].reset_index(drop=True)\n",
    "    return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "def trim_trailing_empty_rows(df):\n",
    "    for i in reversed(range(len(df))):\n",
    "        if not is_row_empty(df.iloc[i]):\n",
    "            return df.iloc[:i+1].reset_index(drop=True)\n",
    "    return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "def find_trial_split_index(df):\n",
    "    for i in range(len(df)):\n",
    "        if is_row_empty(df.iloc[i]):\n",
    "            if i+1 < len(df) and is_row_empty(df.iloc[i+1]):\n",
    "                return i\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def split_trials(df):\n",
    "    df = trim_leading_empty_rows(df)\n",
    "    split_idx = find_trial_split_index(df)\n",
    "    if split_idx is None:\n",
    "        return df.reset_index(drop=True), pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    trial1 = df.iloc[:split_idx]\n",
    "    trial2 = df.iloc[split_idx+1:]\n",
    "\n",
    "    trial1 = trim_trailing_empty_rows(trial1)\n",
    "    trial2 = trim_leading_empty_rows(trial2)\n",
    "    trial2 = trim_trailing_empty_rows(trial2)\n",
    "\n",
    "    return trial1.reset_index(drop=True), trial2.reset_index(drop=True)\n",
    "def safe_resample(df_list, time_col, target_rows):\n",
    "    return [\n",
    "        resample_by_time(df, time_col, target_rows)\n",
    "        for df in df_list\n",
    "        if time_col in df.columns and not df.empty\n",
    "    ]\n",
    "\n",
    "def resample_by_time(df, time_col, target_rows):\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Convert time column to numeric\n",
    "    df[time_col] = pd.to_numeric(df[time_col], errors='coerce')\n",
    "    df = df.dropna(subset=[time_col])\n",
    "    df = df.drop_duplicates(subset=time_col)\n",
    "\n",
    "    # 2. Convert all other columns to numeric\n",
    "    for col in df.columns:\n",
    "        if col != time_col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # 3. Resample\n",
    "    df = df.set_index(time_col)\n",
    "    new_time_index = np.linspace(df.index.min(), df.index.max(), target_rows)\n",
    "    df_resampled = df.reindex(new_time_index)\n",
    "    df_resampled = df_resampled.interpolate(method='linear', axis=0).reset_index()\n",
    "    df_resampled.rename(columns={'index': time_col}, inplace=True)\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "\n",
    "\n",
    "def standardize_df(df, exclude_cols):\n",
    "    scaler = StandardScaler()\n",
    "    cols_to_scale = [col for col in df.columns if col not in exclude_cols]\n",
    "    df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "    return df\n",
    "\n",
    "# === MAIN ===\n",
    "input_folder = r\"C:\\Users\\Abhay\\Downloads\\ExportedSheets\"\n",
    "csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "all_abhay_trial1, all_abhay_trial2 = [], []\n",
    "all_arjun_trial1, all_arjun_trial2 = [], []\n",
    "\n",
    "for file in csv_files:\n",
    "    \n",
    "\n",
    "    path = os.path.join(input_folder, file)\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        df = df.iloc[1:].reset_index(drop=True)\n",
    "        df = df.iloc[:, :39]  # 19 Abhay + 1 blank + 19 Arjun\n",
    "\n",
    "        abhay_df = df.iloc[:, :19].copy()\n",
    "        arjun_df = df.iloc[:, 20:].copy()\n",
    "\n",
    "        abhay_df.columns = [f\"Abhay_{col}\" for col in expected_columns]\n",
    "        arjun_df.columns = [f\"Arjun_{col}\" for col in expected_columns]\n",
    "\n",
    "        abhay_df.dropna(axis=1, how='all', inplace=True)\n",
    "        arjun_df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "        abhay_t1, abhay_t2 = split_trials(abhay_df)\n",
    "        arjun_t1, arjun_t2 = split_trials(arjun_df)\n",
    "\n",
    "\n",
    "        all_abhay_trial1.append(abhay_t1)\n",
    "        all_abhay_trial2.append(abhay_t2)\n",
    "        all_arjun_trial1.append(arjun_t1)\n",
    "        all_arjun_trial2.append(arjun_t2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# === Compute median length ===\n",
    "all_lengths = [len(df) for df in all_abhay_trial1 + all_abhay_trial2 + all_arjun_trial1 + all_arjun_trial2]\n",
    "median_len = int(np.median(all_lengths))\n",
    "print(f\"Using median trial length: {median_len}\")\n",
    "for filename, a1, a2, r1, r2 in zip(csv_files, all_abhay_trial1, all_abhay_trial2, all_arjun_trial1, all_arjun_trial2):\n",
    "    print(f\"{filename}:\")\n",
    "    print(f\"  Abhay Trial 1: {len(a1)} rows\")\n",
    "    print(f\"  Abhay Trial 2: {len(a2)} rows\")\n",
    "    print(f\"  Arjun Trial 1: {len(r1)} rows\")\n",
    "    print(f\"  Arjun Trial 2: {len(r2)} rows\")\n",
    "\n",
    "# === Resample ===\n",
    "all_abhay_trial1 = safe_resample(all_abhay_trial1, 'Abhay_time', median_len)\n",
    "all_abhay_trial2 = safe_resample(all_abhay_trial2, 'Abhay_time', median_len)\n",
    "all_arjun_trial1 = safe_resample(all_arjun_trial1, 'Arjun_time', median_len)\n",
    "all_arjun_trial2 = safe_resample(all_arjun_trial2, 'Arjun_time', median_len)\n",
    "print(\"\\n=== Trial Lengths Per File ===\")\n",
    "\n",
    "\n",
    "# === Standardize ===\n",
    "all_abhay_trial1 = [standardize_df(df, ['Abhay_time']) for df in all_abhay_trial1]\n",
    "all_abhay_trial2 = [standardize_df(df, ['Abhay_time']) for df in all_abhay_trial2]\n",
    "all_arjun_trial1 = [standardize_df(df, ['Arjun_time']) for df in all_arjun_trial1]\n",
    "all_arjun_trial2 = [standardize_df(df, ['Arjun_time']) for df in all_arjun_trial2]\n",
    "\n",
    "\n",
    "def cascade_stack(df, rows):\n",
    "    \"\"\"Stacks columns of the first `rows` rows of df vertically into one long row.\"\"\"\n",
    "    return pd.DataFrame([df.iloc[:rows].to_numpy().T.flatten()])\n",
    "\n",
    "# === Cascading ===\n",
    "train_abhay1_df, test_abhay1_df = [], []\n",
    "train_abhay2_df, test_abhay2_df = [], []\n",
    "train_arjun1_df, test_arjun1_df = [], []\n",
    "train_arjun2_df, test_arjun2_df = [], []\n",
    "\n",
    "for a1, a2, r1, r2 in zip(all_abhay_trial1, all_abhay_trial2, all_arjun_trial1, all_arjun_trial2):\n",
    "    # Abhay Trials\n",
    "    train_abhay1_df.append(cascade_stack(a1, 70))\n",
    "    test_abhay1_df.append(cascade_stack(a1, 30))\n",
    "    train_abhay2_df.append(cascade_stack(a2, 70))\n",
    "    test_abhay2_df.append(cascade_stack(a2, 30))\n",
    "    \n",
    "    # Arjun Trials\n",
    "    train_arjun1_df.append(cascade_stack(r1, 70))\n",
    "    test_arjun1_df.append(cascade_stack(r1, 30))\n",
    "    train_arjun2_df.append(cascade_stack(r2, 70))\n",
    "    test_arjun2_df.append(cascade_stack(r2, 30))\n",
    "\n",
    "# === Combine to single DataFrames ===\n",
    "train_abhay1_df = pd.concat(train_abhay1_df, ignore_index=True)\n",
    "train_abhay2_df = pd.concat(train_abhay2_df, ignore_index=True)\n",
    "train_arjun1_df = pd.concat(train_arjun1_df, ignore_index=True)\n",
    "train_arjun2_df = pd.concat(train_arjun2_df, ignore_index=True)\n",
    "\n",
    "test_abhay1_df = pd.concat(test_abhay1_df, ignore_index=True)\n",
    "test_abhay2_df = pd.concat(test_abhay2_df, ignore_index=True)\n",
    "test_arjun1_df = pd.concat(test_arjun1_df, ignore_index=True)\n",
    "test_arjun2_df = pd.concat(test_arjun2_df, ignore_index=True)\n",
    "\n",
    "print(\"✅ Cascading complete\")\n",
    "print(f\"Abhay Train 1: {train_abhay1_df.shape}, Test 1: {test_abhay1_df.shape}\")\n",
    "print(f\"Arjun Train 1: {train_arjun1_df.shape}, Test 1: {test_arjun1_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca56917-b813-4137-a5b8-0c761dbd9bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
